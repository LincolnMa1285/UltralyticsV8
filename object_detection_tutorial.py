# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

"""
YOLOv8 ç›®æ ‡æ£€æµ‹å®Œæ•´ç¤ºä¾‹

æœ¬è„šæœ¬æ¼”ç¤ºäº†ä½¿ç”¨ Ultralytics YOLOv8 è¿›è¡Œç›®æ ‡æ£€æµ‹çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. æ¨¡å‹åŠ è½½ä¸åˆå§‹åŒ–
2. å•å¼ å›¾åƒé¢„æµ‹
3. æ‰¹é‡å›¾åƒé¢„æµ‹
4. è§†é¢‘æ£€æµ‹
5. å®æ—¶æ‘„åƒå¤´æ£€æµ‹
6. æ¨¡å‹è®­ç»ƒ
7. æ¨¡å‹éªŒè¯
8. æ¨¡å‹å¯¼å‡º

ä½¿ç”¨æ–¹æ³•ï¼š
    python object_detection_tutorial.py --mode predict --source path/to/image.jpg
    python object_detection_tutorial.py --mode train --data coco8.yaml
    python object_detection_tutorial.py --mode video --source path/to/video.mp4
    python object_detection_tutorial.py --mode webcam --source 0
"""

import argparse
from pathlib import Path

import cv2
import torch
from ultralytics import YOLO
from ultralytics.utils import ASSETS, ROOT, SETTINGS


class ObjectDetector:
    """
    YOLOv8 ç›®æ ‡æ£€æµ‹å™¨ç±»
    
    è¯¥ç±»å°è£…äº† YOLOv8 æ¨¡å‹çš„å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬é¢„æµ‹ã€è®­ç»ƒã€éªŒè¯å’Œå¯¼å‡ºã€‚
    
    å±æ€§:
        model (YOLO): YOLOv8 æ¨¡å‹å®ä¾‹
        device (str): è¿è¡Œè®¾å¤‡ ('cpu', 'cuda', 'mps' ç­‰)
        
    æ–¹æ³•:
        predict_image: å¯¹å•å¼ å›¾åƒè¿›è¡Œç›®æ ‡æ£€æµ‹
        predict_batch: å¯¹å¤šå¼ å›¾åƒè¿›è¡Œæ‰¹é‡æ£€æµ‹
        predict_video: å¯¹è§†é¢‘æ–‡ä»¶è¿›è¡Œæ£€æµ‹
        predict_webcam: ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œå®æ—¶æ£€æµ‹
        train: è®­ç»ƒæ¨¡å‹
        validate: éªŒè¯æ¨¡å‹æ€§èƒ½
        export: å¯¼å‡ºæ¨¡å‹åˆ°å…¶ä»–æ ¼å¼
    """
    
    def __init__(self, model_path="yolov8n.pt", device=""):
        """
        åˆå§‹åŒ–ç›®æ ‡æ£€æµ‹å™¨
        
        å‚æ•°:
            model_path (str): æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ YOLOv8n
            device (str): è¿è¡Œè®¾å¤‡ï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.model = YOLO(model_path)
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def predict_image(self, source, conf=0.25, iou=0.7, save=True, show=False):
        """
        å¯¹å•å¼ æˆ–å¤šå¼ å›¾åƒè¿›è¡Œç›®æ ‡æ£€æµ‹
        
        å‚æ•°:
            source (str): å›¾åƒè·¯å¾„æˆ–å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            conf (float): ç½®ä¿¡åº¦é˜ˆå€¼
            iou (float): NMS IOU é˜ˆå€¼
            save (bool): æ˜¯å¦ä¿å­˜ç»“æœ
            show (bool): æ˜¯å¦æ˜¾ç¤ºç»“æœ
            
        è¿”å›:
            results: æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        print(f"\nå¼€å§‹å›¾åƒæ£€æµ‹: {source}")
        results = self.model.predict(
            source=source,
            conf=conf,
            iou=iou,
            save=save,
            show=show,
            device=self.device
        )
        
        for i, result in enumerate(results):
            print(f"\nå›¾åƒ {i+1} æ£€æµ‹ç»“æœ:")
            if result.boxes is not None:
                print(f"  æ£€æµ‹åˆ° {len(result.boxes)} ä¸ªç›®æ ‡")
                if len(result.boxes) > 0:
                    for box in result.boxes:
                        cls_id = int(box.cls[0])
                        conf_score = float(box.conf[0])
                        cls_name = result.names[cls_id]
                        print(f"  - {cls_name}: {conf_score:.2f}")
            else:
                print(f"  æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼ˆboxes ä¸º Noneï¼‰")
                    
        return results
    
    def predict_video(self, source, conf=0.25, iou=0.7, save=True, show=False):
        """
        å¯¹è§†é¢‘æ–‡ä»¶è¿›è¡Œç›®æ ‡æ£€æµ‹
        
        å‚æ•°:
            source (str): è§†é¢‘æ–‡ä»¶è·¯å¾„
            conf (float): ç½®ä¿¡åº¦é˜ˆå€¼
            iou (float): NMS IOU é˜ˆå€¼
            save (bool): æ˜¯å¦ä¿å­˜ç»“æœè§†é¢‘
            show (bool): æ˜¯å¦å®æ—¶æ˜¾ç¤ºç»“æœ
        """
        print(f"\nå¼€å§‹è§†é¢‘æ£€æµ‹: {source}")
        results = self.model.predict(
            source=source,
            conf=conf,
            iou=iou,
            save=save,
            show=show,
            stream=True,
            device=self.device
        )
        
        frame_count = 0
        for result in results:
            frame_count += 1
            if frame_count % 30 == 0:
                num_boxes = len(result.boxes) if result.boxes is not None else 0
                print(f"å·²å¤„ç† {frame_count} å¸§, å½“å‰å¸§æ£€æµ‹åˆ° {num_boxes} ä¸ªç›®æ ‡")
                
        print(f"\nè§†é¢‘æ£€æµ‹å®Œæˆï¼Œå…±å¤„ç† {frame_count} å¸§")
    
    def predict_webcam(self, source=0, conf=0.25, iou=0.7):
        """
        ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œå®æ—¶ç›®æ ‡æ£€æµ‹
        
        å‚æ•°:
            source (int): æ‘„åƒå¤´ç¼–å·ï¼Œé»˜è®¤ 0 è¡¨ç¤ºé»˜è®¤æ‘„åƒå¤´
            conf (float): ç½®ä¿¡åº¦é˜ˆå€¼
            iou (float): NMS IOU é˜ˆå€¼
        """
        print(f"\nå¯åŠ¨æ‘„åƒå¤´æ£€æµ‹ (æŒ‰ 'q' é”®é€€å‡º)")
        results = self.model.predict(
            source=source,
            conf=conf,
            iou=iou,
            show=True,
            stream=True,
            device=self.device
        )
        
        for result in results:
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
        print("\næ‘„åƒå¤´æ£€æµ‹ç»“æŸ")
    
    def train(self, data, epochs=100, imgsz=640, batch=16, project=None, name="exp"):
        """
        è®­ç»ƒ YOLOv8 æ¨¡å‹
        
        å‚æ•°:
            data (str): æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (YAML)
            epochs (int): è®­ç»ƒè½®æ•°
            imgsz (int): è¾“å…¥å›¾åƒå°ºå¯¸
            batch (int): æ‰¹æ¬¡å¤§å°
            project (str): é¡¹ç›®ä¿å­˜è·¯å¾„
            name (str): å®éªŒåç§°
            
        è¿”å›:
            results: è®­ç»ƒç»“æœ
        """
        print(f"\nå¼€å§‹è®­ç»ƒæ¨¡å‹")
        print(f"  æ•°æ®é›†: {data}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  å›¾åƒå°ºå¯¸: {imgsz}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch}")
        
        results = self.model.train(
            data=data,
            epochs=epochs,
            imgsz=imgsz,
            batch=batch,
            device=self.device,
            project=project,
            name=name
        )
        
        print(f"\nè®­ç»ƒå®Œæˆ!")
        return results
    
    def validate(self, data=None, split="val", imgsz=640):
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
            data (str): æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒNone è¡¨ç¤ºä½¿ç”¨è®­ç»ƒæ—¶çš„æ•°æ®é›†
            split (str): æ•°æ®é›†åˆ’åˆ† ('val', 'test')
            imgsz (int): è¾“å…¥å›¾åƒå°ºå¯¸
            
        è¿”å›:
            metrics: éªŒè¯æŒ‡æ ‡
        """
        print(f"\nå¼€å§‹éªŒè¯æ¨¡å‹")
        metrics = self.model.val(
            data=data,
            split=split,
            imgsz=imgsz,
            device=self.device
        )
        
        print(f"\néªŒè¯ç»“æœ:")
        print(f"  mAP50-95: {metrics.box.map:.4f}")
        print(f"  mAP50: {metrics.box.map50:.4f}")
        print(f"  mAP75: {metrics.box.map75:.4f}")
        
        return metrics
    
    def export(self, format="onnx", imgsz=640, half=False, simplify=True):
        """
        å¯¼å‡ºæ¨¡å‹åˆ°å…¶ä»–æ ¼å¼
        
        å‚æ•°:
            format (str): å¯¼å‡ºæ ¼å¼ ('onnx', 'torchscript', 'openvino', 'engine', 'coreml', 'tflite' ç­‰)
            imgsz (int): è¾“å…¥å›¾åƒå°ºå¯¸
            half (bool): æ˜¯å¦ä½¿ç”¨ FP16 åŠç²¾åº¦
            simplify (bool): æ˜¯å¦ç®€åŒ– ONNX æ¨¡å‹
            
        è¿”å›:
            export_path: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"\nå¼€å§‹å¯¼å‡ºæ¨¡å‹åˆ° {format.upper()} æ ¼å¼")
        export_path = self.model.export(
            format=format,
            imgsz=imgsz,
            half=half,
            simplify=simplify
        )
        
        print(f"\nå¯¼å‡ºå®Œæˆ: {export_path}")
        return export_path


def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""
    parser = argparse.ArgumentParser(description="YOLOv8 ç›®æ ‡æ£€æµ‹å®Œæ•´ç¤ºä¾‹")
    
    parser.add_argument("--mode", type=str, default="predict", 
                       choices=["predict", "train", "validate", "export", "video", "webcam"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--model", type=str, default="yolov8n.pt",
                       help="æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--source", type=str, default=str(ASSETS / "bus.jpg"),
                       help="è¾“å…¥æº (å›¾åƒè·¯å¾„ã€è§†é¢‘è·¯å¾„ã€æ‘„åƒå¤´ç¼–å·)")
    parser.add_argument("--data", type=str, default="coco8.yaml",
                       help="æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (è®­ç»ƒ/éªŒè¯æ—¶ä½¿ç”¨)")
    parser.add_argument("--conf", type=float, default=0.25,
                       help="ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--iou", type=float, default=0.7,
                       help="NMS IOU é˜ˆå€¼")
    parser.add_argument("--device", type=str, default="",
                       help="è¿è¡Œè®¾å¤‡ (cpu, cuda, mps ç­‰)")
    parser.add_argument("--epochs", type=int, default=100,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--imgsz", type=int, default=640,
                       help="è¾“å…¥å›¾åƒå°ºå¯¸")
    parser.add_argument("--batch", type=int, default=16,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--format", type=str, default="onnx",
                       help="å¯¼å‡ºæ ¼å¼ (onnx, torchscript, openvino, engine, coreml, tflite ç­‰)")
    parser.add_argument("--save", action="store_true", default=True,
                       help="æ˜¯å¦ä¿å­˜ç»“æœ")
    parser.add_argument("--show", action="store_true",
                       help="æ˜¯å¦æ˜¾ç¤ºç»“æœ")
    
    args = parser.parse_args()
    
    detector = ObjectDetector(model_path=args.model, device=args.device)
    
    if args.mode == "predict":
        detector.predict_image(
            source=args.source,
            conf=args.conf,
            iou=args.iou,
            save=args.save,
            show=args.show
        )
        
    elif args.mode == "video":
        detector.predict_video(
            source=args.source,
            conf=args.conf,
            iou=args.iou,
            save=args.save,
            show=args.show
        )
        
    elif args.mode == "webcam":
        source = int(args.source) if args.source.isdigit() else args.source
        detector.predict_webcam(
            source=source,
            conf=args.conf,
            iou=args.iou
        )
        
    elif args.mode == "train":
        detector.train(
            data=args.data,
            epochs=args.epochs,
            imgsz=args.imgsz,
            batch=args.batch
        )
        
    elif args.mode == "validate":
        detector.validate(
            data=args.data,
            imgsz=args.imgsz
        )
        
    elif args.mode == "export":
        detector.export(
            format=args.format,
            imgsz=args.imgsz
        )
        
    print("\nä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()